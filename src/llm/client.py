# llm/client.py

import logging
from typing import Callable
from huggingface_hub import InferenceApi
from threading import Thread
from huggingface_hub.utils import build_hf_headers
from llm.prompts import PromptManager


class LLMClient:
    """
    Manages interactions with the LLM via Hugging Face Inference API.

    Attributes:
        logger (logging.Logger): Logger for the LLMClient.
        client (InferenceApi): Hugging Face Inference API Client.
        model (str): The model identifier from Hugging Face Hub.
        prompt_manager (PromptManager): Manages prompt templates.
    """

    def __init__(self, api_key: str, model: str):
        """
        Initializes the LLMClient.

        Args:
            api_key (str): The Hugging Face API key for authentication.
            model (str): The identifier of the model to use from Hugging Face Hub.
        """
        self.logger = logging.getLogger(__name__)
        headers = build_hf_headers(token=api_key)  # Manually setting API key
        self.client = InferenceApi(
            repo_id=model,
            token=api_key,
            task='text-generation'  # Explicitly set the task
        )
        # Override the api_url as per forum suggestion
        self.client.api_url = f"https://api-inference.huggingface.co/models/{model}"
        self.model = model
        self.prompt_manager = PromptManager()
        self.logger.info(f"LLMClient initialized with model '{self.model}'.")

    def generate_response(self, prompt: str, max_tokens: int = 500) -> str:
        """
        Generates a response from the LLM based on the provided prompt.

        Args:
            prompt (str): The prompt string to send to the LLM.
            max_tokens (int, optional): The maximum number of tokens in the response.

        Returns:
            str: The response generated by the LLM.
        """
        self.logger.debug(f"Generating LLM response for prompt: {prompt}")

        try:
            # Corrected parameter key from 'params' to 'parameters'
            response = self.client(inputs=prompt, params={"max_new_tokens": max_tokens})
            self.logger.debug(f"Received raw LLM response: {response}")
            print(f"Received raw LLM response: {response}")

            # Handle different response formats
            if isinstance(response, list) and len(response) > 0:
                first_response = response[0]
                if isinstance(first_response, dict) and 'generated_text' in first_response:
                    generated_text = first_response['generated_text']
                else:
                    self.logger.error(f"Unexpected response format: {response}")
                    return "rest"
            elif isinstance(response, dict):
                if 'generated_text' in response:
                    generated_text = response['generated_text']
                elif 'text' in response:
                    generated_text = response['text']
                else:
                    self.logger.error(f"Unexpected response format: {response}")
                    return "rest"
            else:
                self.logger.error(f"Unexpected response format: {response}")
                return "rest"

            self.logger.debug(f"Processed LLM response: {generated_text}")
            return generated_text.strip()
        except Exception as e:
            self.logger.error(f"LLM generation failed: {e}")
            return "rest"  # Default action in case of failure

    def generate_response_async(self, prompt: str, callback: Callable[[str], None], max_tokens: int = 500):
        """
        Generates a response from the LLM asynchronously based on the provided prompt.

        Args:
            prompt (str): The prompt string to send to the LLM.
            callback (Callable[[str], None]): The function to call with the response once it's ready.
            max_tokens (int, optional): The maximum number of tokens in the response.

        Returns:
            None
        """
        def run():
            self.logger.debug(f"Starting async LLM generation for prompt: {prompt}")
            response = self.generate_response(prompt, max_tokens)
            callback(response)

        thread = Thread(target=run)
        thread.start()
